{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4920\n",
      "Number of classes: 41\n",
      "Class distribution:\n",
      " Disease\n",
      "Fungal infection                           120\n",
      "Hepatitis C                                120\n",
      "Hepatitis E                                120\n",
      "Alcoholic hepatitis                        120\n",
      "Tuberculosis                               120\n",
      "Common Cold                                120\n",
      "Pneumonia                                  120\n",
      "Dimorphic hemorrhoids (piles)              120\n",
      "Heart attack                               120\n",
      "Varicose veins                             120\n",
      "Hypothyroidism                             120\n",
      "Hyperthyroidism                            120\n",
      "Hypoglycemia                               120\n",
      "Osteoarthristis                            120\n",
      "Arthritis                                  120\n",
      "(vertigo) Paroymsal  Positional Vertigo    120\n",
      "Acne                                       120\n",
      "Urinary tract infection                    120\n",
      "Psoriasis                                  120\n",
      "Hepatitis D                                120\n",
      "Hepatitis B                                120\n",
      "Allergy                                    120\n",
      "hepatitis A                                120\n",
      "GERD                                       120\n",
      "Chronic cholestasis                        120\n",
      "Drug Reaction                              120\n",
      "Peptic ulcer disease                       120\n",
      "AIDS                                       120\n",
      "Diabetes                                   120\n",
      "Gastroenteritis                            120\n",
      "Bronchial Asthma                           120\n",
      "Hypertension                               120\n",
      "Migraine                                   120\n",
      "Cervical spondylosis                       120\n",
      "Paralysis (brain hemorrhage)               120\n",
      "Jaundice                                   120\n",
      "Malaria                                    120\n",
      "Chicken pox                                120\n",
      "Dengue                                     120\n",
      "Typhoid                                    120\n",
      "Impetigo                                   120\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "input_file = \"DiseaseAndSymptoms.csv\"\n",
    "data = pd.read_csv(input_file)\n",
    "data.columns = [col.replace(\"_\", \" \") for col in data.columns]\n",
    "data = data.apply(lambda x: x.str.replace(\"_\", \" \") if x.dtype == \"object\" else x)\n",
    "data[\"Disease\"] = data[\"Disease\"].replace(\"Peptic ulcer diseae\", \"Peptic ulcer disease\")\n",
    "data[\"Disease\"] = data[\"Disease\"].replace(\"Dimorphic hemmorhoids(piles)\", \"Dimorphic hemorrhoids (piles)\")\n",
    "\n",
    "# Clean and combine symptoms\n",
    "symptom_cols = [col for col in data.columns if \"Symptom\" in col]\n",
    "data[\"Symptoms\"] = data[symptom_cols].apply(\n",
    "    lambda row: \" \".join(sorted(set([s.strip() for s in row if pd.notna(s)]))), axis=1\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(f\"Number of rows: {len(data)}\")\n",
    "disease_list = sorted(data[\"Disease\"].unique())\n",
    "num_classes = len(disease_list)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Class distribution:\\n\", data[\"Disease\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3936, Validation samples: 984\n"
     ]
    }
   ],
   "source": [
    "# Split data (stratified)\n",
    "X_train, X_val, y_train_labels, y_val_labels = train_test_split(\n",
    "    data[\"Symptoms\"], data[\"Disease\"], test_size=0.2, random_state=42, stratify=data[\"Disease\"]\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (3936, 41), Validation labels shape: (984, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to categorical\n",
    "y_train = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_train_labels], num_classes=num_classes)\n",
    "y_val = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_val_labels], num_classes=num_classes)\n",
    "print(f\"Training labels shape: {y_train.shape}, Validation labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training samples: 256584, Validation samples: 64146\n",
      "Augmented training embeddings shape: (256584, 200)\n",
      "Augmented validation embeddings shape: (64146, 200)\n",
      "Augmented training labels shape: (256584, 41), Validation labels shape: (64146, 41)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load BioWordVec embeddings (already loaded in Cell 5)\n",
    "word2vec_path = r\"C:\\Users\\ACER\\Downloads\\bio_embedding_extrinsic.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "# Function to get symptom embedding (already defined in Cell 5)\n",
    "def get_symptom_embedding(text, wv):\n",
    "    words = text.split()\n",
    "    vectors = [wv[word] for word in words if word in wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(wv.vector_size)\n",
    "\n",
    "# Function to generate partial symptom combinations\n",
    "def generate_partial_symptoms(symptom_text, min_symptoms=2):\n",
    "    symptoms = symptom_text.split()\n",
    "    partial_combinations = []\n",
    "    for n in range(min_symptoms, len(symptoms) + 1):\n",
    "        for i in range(5):  # Generate 5 partial combinations per length\n",
    "            partial = \" \".join(sorted(random.sample(symptoms, n)))\n",
    "            partial_combinations.append(partial)\n",
    "    return partial_combinations\n",
    "\n",
    "# Augment training data\n",
    "augmented_data = []\n",
    "for idx, row in data.iterrows():\n",
    "    symptom_text = row[\"Symptoms\"]\n",
    "    disease = row[\"Disease\"]\n",
    "    # Add the full symptom combination\n",
    "    augmented_data.append((symptom_text, disease))\n",
    "    # Add partial symptom combinations\n",
    "    partial_symptoms = generate_partial_symptoms(symptom_text)\n",
    "    for partial in partial_symptoms:\n",
    "        augmented_data.append((partial, disease))\n",
    "\n",
    "# Create augmented DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=[\"Symptoms\", \"Disease\"])\n",
    "\n",
    "# Split augmented data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_aug, X_val_aug, y_train_labels_aug, y_val_labels_aug = train_test_split(\n",
    "    augmented_df[\"Symptoms\"], augmented_df[\"Disease\"], test_size=0.2, random_state=42, stratify=augmented_df[\"Disease\"]\n",
    ")\n",
    "print(f\"Augmented training samples: {len(X_train_aug)}, Validation samples: {len(X_val_aug)}\")\n",
    "\n",
    "# Generate embeddings for augmented data\n",
    "X_train_emb_aug = np.array([get_symptom_embedding(text, word_vectors) for text in X_train_aug])\n",
    "X_val_emb_aug = np.array([get_symptom_embedding(text, word_vectors) for text in X_val_aug])\n",
    "print(f\"Augmented training embeddings shape: {X_train_emb_aug.shape}\")\n",
    "print(f\"Augmented validation embeddings shape: {X_val_emb_aug.shape}\")\n",
    "\n",
    "# Convert labels to categorical\n",
    "disease_list = sorted(data[\"Disease\"].unique())\n",
    "num_classes = len(disease_list)\n",
    "y_train_aug = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_train_labels_aug], num_classes=num_classes)\n",
    "y_val_aug = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_val_labels_aug], num_classes=num_classes)\n",
    "print(f\"Augmented training labels shape: {y_train_aug.shape}, Validation labels shape: {y_val_aug.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9173 - loss: 0.3061\n",
      "Epoch 1: val_accuracy improved from -inf to 0.95686, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.9173 - loss: 0.3061 - val_accuracy: 0.9569 - val_loss: 0.1270\n",
      "Epoch 2/20\n",
      "\u001b[1m8009/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9461 - loss: 0.1744\n",
      "Epoch 2: val_accuracy did not improve from 0.95686\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.9461 - loss: 0.1743 - val_accuracy: 0.9564 - val_loss: 0.1234\n",
      "Epoch 3/20\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9492 - loss: 0.1552\n",
      "Epoch 3: val_accuracy improved from 0.95686 to 0.95736, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.9492 - loss: 0.1552 - val_accuracy: 0.9574 - val_loss: 0.1174\n",
      "Epoch 4/20\n",
      "\u001b[1m8011/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9514 - loss: 0.1436\n",
      "Epoch 4: val_accuracy improved from 0.95736 to 0.95880, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.9514 - loss: 0.1436 - val_accuracy: 0.9588 - val_loss: 0.1103\n",
      "Epoch 5/20\n",
      "\u001b[1m8018/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9521 - loss: 0.1388\n",
      "Epoch 5: val_accuracy did not improve from 0.95880\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.9521 - loss: 0.1388 - val_accuracy: 0.9586 - val_loss: 0.1108\n",
      "Epoch 6/20\n",
      "\u001b[1m8013/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9527 - loss: 0.1359\n",
      "Epoch 6: val_accuracy improved from 0.95880 to 0.96039, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.9527 - loss: 0.1359 - val_accuracy: 0.9604 - val_loss: 0.1042\n",
      "Epoch 7/20\n",
      "\u001b[1m8006/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1275\n",
      "Epoch 7: val_accuracy did not improve from 0.96039\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.9553 - loss: 0.1275 - val_accuracy: 0.9597 - val_loss: 0.1049\n",
      "Epoch 8/20\n",
      "\u001b[1m8012/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9543 - loss: 0.1286\n",
      "Epoch 8: val_accuracy did not improve from 0.96039\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.9543 - loss: 0.1286 - val_accuracy: 0.9598 - val_loss: 0.1042\n",
      "Epoch 9/20\n",
      "\u001b[1m8015/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9552 - loss: 0.1253\n",
      "Epoch 9: val_accuracy improved from 0.96039 to 0.96040, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.9552 - loss: 0.1253 - val_accuracy: 0.9604 - val_loss: 0.1048\n",
      "Epoch 10/20\n",
      "\u001b[1m8014/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9545 - loss: 0.1252\n",
      "Epoch 10: val_accuracy did not improve from 0.96040\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.9545 - loss: 0.1252 - val_accuracy: 0.9598 - val_loss: 0.1016\n",
      "Epoch 11/20\n",
      "\u001b[1m8018/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9557 - loss: 0.1226\n",
      "Epoch 11: val_accuracy did not improve from 0.96040\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.9557 - loss: 0.1226 - val_accuracy: 0.9602 - val_loss: 0.1024\n",
      "Epoch 12/20\n",
      "\u001b[1m8003/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1212\n",
      "Epoch 12: val_accuracy did not improve from 0.96040\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.9562 - loss: 0.1212 - val_accuracy: 0.9603 - val_loss: 0.1010\n",
      "Epoch 13/20\n",
      "\u001b[1m8003/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9566 - loss: 0.1196\n",
      "Epoch 13: val_accuracy did not improve from 0.96040\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1196 - val_accuracy: 0.9601 - val_loss: 0.1013\n",
      "Epoch 14/20\n",
      "\u001b[1m8012/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9554 - loss: 0.1213\n",
      "Epoch 14: val_accuracy did not improve from 0.96040\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.9554 - loss: 0.1213 - val_accuracy: 0.9601 - val_loss: 0.1004\n",
      "Epoch 15/20\n",
      "\u001b[1m8008/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9565 - loss: 0.1187\n",
      "Epoch 15: val_accuracy improved from 0.96040 to 0.96043, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.9565 - loss: 0.1187 - val_accuracy: 0.9604 - val_loss: 0.1003\n",
      "Epoch 16/20\n",
      "\u001b[1m8010/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9571 - loss: 0.1181\n",
      "Epoch 16: val_accuracy did not improve from 0.96043\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.9571 - loss: 0.1181 - val_accuracy: 0.9601 - val_loss: 0.1001\n",
      "Epoch 17/20\n",
      "\u001b[1m8002/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1183\n",
      "Epoch 17: val_accuracy did not improve from 0.96043\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1183 - val_accuracy: 0.9601 - val_loss: 0.0998\n",
      "Epoch 18/20\n",
      "\u001b[1m8009/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9567 - loss: 0.1183\n",
      "Epoch 18: val_accuracy improved from 0.96043 to 0.96182, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.9567 - loss: 0.1183 - val_accuracy: 0.9618 - val_loss: 0.0991\n",
      "Epoch 19/20\n",
      "\u001b[1m8018/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.1175\n",
      "Epoch 19: val_accuracy improved from 0.96182 to 0.96188, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.1175 - val_accuracy: 0.9619 - val_loss: 0.0956\n",
      "Epoch 20/20\n",
      "\u001b[1m8010/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1163\n",
      "Epoch 20: val_accuracy did not improve from 0.96188\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1163 - val_accuracy: 0.9608 - val_loss: 0.0999\n",
      "Restoring model weights from the end of the best epoch: 19.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\"biowordvec_diagnosis_model.keras\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\", verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train_emb_aug, y_train_aug,\n",
    "    validation_data=(X_val_emb_aug, y_val_aug),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved symptom embeddings to symptom_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "# symptom_texts = data[\"Symptoms\"].tolist()\n",
    "# embeddings = np.array([get_symptom_embedding(text, word_vectors) for text in symptom_texts])\n",
    "\n",
    "# # Save embeddings to .npy file\n",
    "# embedding_dict = {\"symptoms\": symptom_texts, \"embeddings\": embeddings}\n",
    "# npy_path = \"symptom_embeddings.npy\"\n",
    "# np.save(npy_path, embedding_dict)\n",
    "# print(f\"Saved symptom embeddings to {npy_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
