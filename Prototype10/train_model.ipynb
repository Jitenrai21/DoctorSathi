{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381fcb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 1: Import Libraries and Define Paths\n",
    "#\n",
    "# In this cell, we import the necessary libraries and define the file paths for the dataset, BioWordVec embeddings, and output files.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Paths to files (adjust as needed)\n",
    "DATA_PATH = \"DiseaseAndSymptoms.csv\"\n",
    "BIOWORDVEC_PATH = r\"C:\\Users\\ACER\\Downloads\\BioWordVec_PubMed_MIMICIII_d200.vec.bin\" # Path to BioWordVec embeddings in .bin format\n",
    "MODEL_PATH = \"biowordvec_diagnosis_model.keras\"\n",
    "TOKENIZER_PATH = \"symptom_tokenizer.pkl\"\n",
    "LABEL_ENCODER_PATH = \"label_encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e63739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Dataset size after deduplication: 304\n",
      "Training dataset size: 243, Test dataset size: 61\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 2: Load and Preprocess the Dataset\n",
    "#\n",
    "# This cell loads the dataset (`DiseaseAndSymptoms.csv`), preprocesses it, and splits it into train/test sets.\n",
    "\n",
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"\n",
    "    Load the dataset, preprocess symptoms, and split into train/test sets.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Peptic ulcer diseae\", \"Peptic ulcer disease\")\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Dimorphic hemmorhoids(piles)\", \"Dimorphic hemorrhoids (piles)\")\n",
    "    \n",
    "    # Replace underscores with spaces in column names and data\n",
    "    data.columns = [col.replace(\"_\", \" \") for col in data.columns]\n",
    "    data = data.apply(lambda x: x.str.replace(\"_\", \" \") if x.dtype == \"object\" else x)\n",
    "    \n",
    "    # Combine all symptoms into a single string per row\n",
    "    symptom_cols = [col for col in data.columns if \"Symptom\" in col]\n",
    "    data[\"Symptoms\"] = data[symptom_cols].apply(\n",
    "        lambda row: \" \".join(sorted(set([s.strip() for s in row if pd.notna(s)]))), axis=1\n",
    "    )\n",
    "    \n",
    "    # Drop rows with empty symptoms\n",
    "    data = data[data[\"Symptoms\"].str.strip() != \"\"]\n",
    "    \n",
    "    data = data.drop_duplicates(subset=[\"Symptoms\", \"Disease\"])\n",
    "    print(f\"Dataset size after deduplication: {len(data)}\")\n",
    "    \n",
    "    # Split into train and test sets (before augmentation)\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_data[[\"Disease\", \"Symptoms\"]], test_data[[\"Disease\", \"Symptoms\"]]\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "train_data, test_data = load_and_preprocess_data(DATA_PATH)\n",
    "print(f\"Training dataset size: {len(train_data)}, Test dataset size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359ee3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping symptom strings between train and test: 0\n"
     ]
    }
   ],
   "source": [
    "train_symptoms = set(train_data[\"Symptoms\"])\n",
    "test_symptoms = set(test_data[\"Symptoms\"])\n",
    "overlap = train_symptoms.intersection(test_symptoms)\n",
    "print(f\"Number of overlapping symptom strings between train and test: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(\"Sample overlapping symptoms:\", list(overlap)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77c3d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training data...\n",
      "Original training dataset size: 243, Augmented training dataset size: 1063\n"
     ]
    }
   ],
   "source": [
    "def augment_data(data, samples_per_disease=20):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating synthetic symptom combinations with co-occurrence patterns.\n",
    "    \"\"\"\n",
    "    augmented_data = data.copy()\n",
    "    diseases = data[\"Disease\"].unique()\n",
    "    \n",
    "    for disease in diseases:\n",
    "        disease_rows = data[data[\"Disease\"] == disease]\n",
    "        all_symptoms = set()\n",
    "        for symptoms in disease_rows[\"Symptoms\"]:\n",
    "            all_symptoms.update(symptoms.split())\n",
    "        all_symptoms = list(all_symptoms)\n",
    "        \n",
    "        # Calculate symptom co-occurrence (simple frequency-based probability)\n",
    "        symptom_counts = {}\n",
    "        for symptoms in disease_rows[\"Symptoms\"]:\n",
    "            symptom_list = symptoms.split()\n",
    "            for i, s1 in enumerate(symptom_list):\n",
    "                if s1 not in symptom_counts:\n",
    "                    symptom_counts[s1] = {}\n",
    "                for s2 in symptom_list[i+1:]:\n",
    "                    if s2 not in symptom_counts[s1]:\n",
    "                        symptom_counts[s1][s2] = 0\n",
    "                    symptom_counts[s1][s2] += 1\n",
    "        \n",
    "        # Normalize to get probabilities\n",
    "        symptom_probs = {}\n",
    "        for s1 in symptom_counts:\n",
    "            total = sum(symptom_counts[s1].values())\n",
    "            if total > 0:\n",
    "                symptom_probs[s1] = {s2: count/total for s2, count in symptom_counts[s1].items()}\n",
    "        \n",
    "        # Generate synthetic samples\n",
    "        for _ in range(samples_per_disease):\n",
    "            num_symptoms = np.random.randint(2, min(len(all_symptoms) + 1, 6))\n",
    "            selected_symptoms = []\n",
    "            # Start with a random symptom\n",
    "            current_symptom = np.random.choice(all_symptoms)\n",
    "            selected_symptoms.append(current_symptom)\n",
    "            \n",
    "            # Add symptoms based on co-occurrence probabilities\n",
    "            for _ in range(num_symptoms - 1):\n",
    "                if current_symptom in symptom_probs and symptom_probs[current_symptom]:\n",
    "                    next_symptom_probs = symptom_probs[current_symptom]\n",
    "                    next_symptom = np.random.choice(\n",
    "                        list(next_symptom_probs.keys()),\n",
    "                        p=list(next_symptom_probs.values())\n",
    "                    )\n",
    "                    selected_symptoms.append(next_symptom)\n",
    "                    current_symptom = next_symptom\n",
    "                else:\n",
    "                    # Fallback to random selection if no co-occurrence data\n",
    "                    remaining_symptoms = [s for s in all_symptoms if s not in selected_symptoms]\n",
    "                    if remaining_symptoms:\n",
    "                        next_symptom = np.random.choice(remaining_symptoms)\n",
    "                        selected_symptoms.append(next_symptom)\n",
    "                        current_symptom = next_symptom\n",
    "            \n",
    "            synthetic_symptoms = \" \".join(sorted(set(selected_symptoms)))\n",
    "            augmented_data = pd.concat([augmented_data, pd.DataFrame({\n",
    "                \"Disease\": [disease],\n",
    "                \"Symptoms\": [synthetic_symptoms]\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    return augmented_data\n",
    "print(\"Augmenting training data...\")\n",
    "augmented_train_data = augment_data(train_data, samples_per_disease=20)\n",
    "print(f\"Original training dataset size: {len(train_data)}, Augmented training dataset size: {len(augmented_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c5616d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "Training samples: 850, Validation samples: 213, Test samples: 61\n",
      "Vocabulary size: 209, Number of classes: 41\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 4: Prepare Data for Training\n",
    "#\n",
    "# This cell tokenizes the symptoms, encodes the diseases, and prepares the training, validation, and test sets.\n",
    "\n",
    "def prepare_data(train_data, test_data, max_len=20):\n",
    "    \"\"\"\n",
    "    Tokenize symptoms, encode diseases, and prepare training, validation, and test data.\n",
    "    \"\"\"\n",
    "    # Tokenize symptoms\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data[\"Symptoms\"])\n",
    "    \n",
    "    # Convert symptoms to sequences\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_data[\"Symptoms\"])\n",
    "    train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    test_sequences = tokenizer.texts_to_sequences(test_data[\"Symptoms\"])\n",
    "    test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    # Encode diseases\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels = label_encoder.fit_transform(train_data[\"Disease\"])\n",
    "    test_labels = label_encoder.transform(test_data[\"Disease\"])\n",
    "    \n",
    "    # Split augmented training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_sequences, train_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Compute class weights to handle class imbalance\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    class_weight_dict = {k: np.sqrt(v) for k, v in class_weight_dict.items()}  # Soften weights\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, test_sequences, test_labels, tokenizer, label_encoder, len(tokenizer.word_index) + 1, class_weight_dict\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "X_train, X_val, y_train, y_val, X_test, y_test, tokenizer, label_encoder, vocab_size, class_weight_dict = prepare_data(augmented_train_data, test_data)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}, Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "682df10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BioWordVec embeddings...\n",
      "Loading BioWordVec embeddings from .bin file...\n",
      "Loaded 16545452 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 5: Load BioWordVec Embeddings\n",
    "#\n",
    "# This cell loads the BioWordVec embeddings from the .bin file using gensim's KeyedVectors.\n",
    "\n",
    "def load_biowordvec_embeddings(biowordvec_path):\n",
    "    \"\"\"\n",
    "    Load BioWordVec embeddings from a .bin file using gensim.\n",
    "    \"\"\"\n",
    "    print(\"Loading BioWordVec embeddings from .bin file...\")\n",
    "    embeddings = KeyedVectors.load_word2vec_format(biowordvec_path, binary=True)\n",
    "    return embeddings\n",
    "\n",
    "print(\"Loading BioWordVec embeddings...\")\n",
    "embeddings_index = load_biowordvec_embeddings(BIOWORDVEC_PATH)\n",
    "print(f\"Loaded {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7afdae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding matrix...\n",
      "Embedding matrix shape: (209, 200)\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 6: Create Embedding Matrix\n",
    "#\n",
    "# This cell creates an embedding matrix using the BioWordVec embeddings for the tokenized vocabulary.\n",
    "\n",
    "def create_embedding_matrix(tokenizer, embeddings_index, vocab_size, embedding_dim=200):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the model using BioWordVec embeddings.\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            # Word not in BioWordVec vocabulary, leave as zero vector\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "print(\"Creating embedding matrix...\")\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, embeddings_index, vocab_size)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc68db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │        \u001b[38;5;34m41,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,800</span> (163.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m41,800\u001b[0m (163.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,800</span> (163.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m41,800\u001b[0m (163.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0166 - loss: 3.7647 - val_accuracy: 0.0423 - val_loss: 3.6919\n",
      "Epoch 2/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0328 - loss: 3.7326 - val_accuracy: 0.0563 - val_loss: 3.6778\n",
      "Epoch 3/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0367 - loss: 3.6801 - val_accuracy: 0.0704 - val_loss: 3.6656\n",
      "Epoch 4/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0513 - loss: 3.6495 - val_accuracy: 0.0845 - val_loss: 3.6415\n",
      "Epoch 5/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0570 - loss: 3.6371 - val_accuracy: 0.0986 - val_loss: 3.6220\n",
      "Epoch 6/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0556 - loss: 3.6195 - val_accuracy: 0.1174 - val_loss: 3.5948\n",
      "Epoch 7/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0427 - loss: 3.6213 - val_accuracy: 0.1362 - val_loss: 3.5753\n",
      "Epoch 8/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0602 - loss: 3.5734 - val_accuracy: 0.1362 - val_loss: 3.5523\n",
      "Epoch 9/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0634 - loss: 3.5680 - val_accuracy: 0.1643 - val_loss: 3.5339\n",
      "Epoch 10/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0851 - loss: 3.5060 - val_accuracy: 0.1690 - val_loss: 3.5021\n",
      "Epoch 11/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0946 - loss: 3.4850 - val_accuracy: 0.1831 - val_loss: 3.4761\n",
      "Epoch 12/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0801 - loss: 3.4902 - val_accuracy: 0.1690 - val_loss: 3.4606\n",
      "Epoch 13/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0773 - loss: 3.4797 - val_accuracy: 0.1831 - val_loss: 3.4419\n",
      "Epoch 14/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1288 - loss: 3.4125 - val_accuracy: 0.2019 - val_loss: 3.4082\n",
      "Epoch 15/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1086 - loss: 3.4227 - val_accuracy: 0.2019 - val_loss: 3.3850\n",
      "Epoch 16/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0929 - loss: 3.4225 - val_accuracy: 0.2254 - val_loss: 3.3638\n",
      "Epoch 17/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1124 - loss: 3.3564 - val_accuracy: 0.2300 - val_loss: 3.3340\n",
      "Epoch 18/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1458 - loss: 3.3302 - val_accuracy: 0.2441 - val_loss: 3.3120\n",
      "Epoch 19/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1406 - loss: 3.2702 - val_accuracy: 0.2488 - val_loss: 3.2910\n",
      "Epoch 20/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1367 - loss: 3.3144 - val_accuracy: 0.2723 - val_loss: 3.2692\n",
      "Epoch 21/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1360 - loss: 3.2696 - val_accuracy: 0.2958 - val_loss: 3.2386\n",
      "Epoch 22/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1437 - loss: 3.2692 - val_accuracy: 0.3052 - val_loss: 3.2148\n",
      "Epoch 23/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1772 - loss: 3.1859 - val_accuracy: 0.2864 - val_loss: 3.1893\n",
      "Epoch 24/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1409 - loss: 3.2601 - val_accuracy: 0.3099 - val_loss: 3.1670\n",
      "Epoch 25/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1449 - loss: 3.2309 - val_accuracy: 0.3099 - val_loss: 3.1491\n",
      "Epoch 26/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1436 - loss: 3.1756 - val_accuracy: 0.3286 - val_loss: 3.1168\n",
      "Epoch 27/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1886 - loss: 3.1577 - val_accuracy: 0.3333 - val_loss: 3.0929\n",
      "Epoch 28/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2169 - loss: 3.0647 - val_accuracy: 0.3615 - val_loss: 3.0810\n",
      "Epoch 29/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1813 - loss: 3.0797 - val_accuracy: 0.3521 - val_loss: 3.0626\n",
      "Epoch 30/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2038 - loss: 3.0171 - val_accuracy: 0.3146 - val_loss: 3.0398\n",
      "Epoch 31/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1593 - loss: 3.0795 - val_accuracy: 0.3380 - val_loss: 3.0253\n",
      "Epoch 32/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1833 - loss: 3.0471 - val_accuracy: 0.3568 - val_loss: 3.0040\n",
      "Epoch 33/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1961 - loss: 3.0263 - val_accuracy: 0.3568 - val_loss: 2.9758\n",
      "Epoch 34/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1975 - loss: 3.0249 - val_accuracy: 0.3803 - val_loss: 2.9632\n",
      "Epoch 35/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2232 - loss: 2.9612 - val_accuracy: 0.3615 - val_loss: 2.9379\n",
      "Epoch 36/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2440 - loss: 2.9184 - val_accuracy: 0.3803 - val_loss: 2.9187\n",
      "Epoch 37/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2405 - loss: 2.9410 - val_accuracy: 0.4038 - val_loss: 2.8951\n",
      "Epoch 38/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2356 - loss: 2.8860 - val_accuracy: 0.3803 - val_loss: 2.8757\n",
      "Epoch 39/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2357 - loss: 2.9270 - val_accuracy: 0.3897 - val_loss: 2.8599\n",
      "Epoch 40/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2438 - loss: 2.8425 - val_accuracy: 0.3897 - val_loss: 2.8401\n",
      "Epoch 41/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2575 - loss: 2.8377 - val_accuracy: 0.4131 - val_loss: 2.8156\n",
      "Epoch 42/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2450 - loss: 2.8557 - val_accuracy: 0.4038 - val_loss: 2.7947\n",
      "Epoch 43/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2789 - loss: 2.7852 - val_accuracy: 0.4038 - val_loss: 2.7764\n",
      "Epoch 44/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2422 - loss: 2.7859 - val_accuracy: 0.4178 - val_loss: 2.7556\n",
      "Epoch 45/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2322 - loss: 2.8902 - val_accuracy: 0.4085 - val_loss: 2.7409\n",
      "Epoch 46/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2305 - loss: 2.8042 - val_accuracy: 0.4272 - val_loss: 2.7257\n",
      "Epoch 47/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2467 - loss: 2.7944 - val_accuracy: 0.4225 - val_loss: 2.7118\n",
      "Epoch 48/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2303 - loss: 2.7832 - val_accuracy: 0.4319 - val_loss: 2.6953\n",
      "Epoch 49/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2610 - loss: 2.7232 - val_accuracy: 0.4319 - val_loss: 2.6619\n",
      "Epoch 50/50\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2620 - loss: 2.7508 - val_accuracy: 0.4413 - val_loss: 2.6402\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 7: Build and Train the Model\n",
    "#\n",
    "# This cell builds a simpler dense model with BioWordVec embeddings and trains it on the augmented dataset.\n",
    "\n",
    "def build_and_train_model(X_train, X_val, y_train, y_val, vocab_size, embedding_matrix, num_classes, class_weight_dict, max_len=20, embedding_dim=200):\n",
    "    \"\"\"\n",
    "    Build and train a simpler dense model with BioWordVec embeddings.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=True),  # Fine-tune embeddings\n",
    "        Flatten(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    # Add early stopping\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with class weights\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,  # Increased\n",
    "        batch_size=32,\n",
    "        callbacks=[EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)],  # Adjusted patience\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Building and training model...\")\n",
    "model, history = build_and_train_model(\n",
    "    X_train, X_val, y_train, y_val, vocab_size, embedding_matrix, num_classes, class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7e4d01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "Test Loss: 1.5290, Test Accuracy: 0.7213\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 8: Evaluate on Test Set\n",
    "#\n",
    "# This cell evaluates the model on the separate test set.\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da4cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model, tokenizer, and label encoder...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 9: Save the Model, Tokenizer, and Label Encoder\n",
    "#\n",
    "# This cell saves the trained model, tokenizer, and label encoder for use in the inference script (`main.py`).\n",
    "\n",
    "print(\"Saving model, tokenizer, and label encoder...\")\n",
    "model.save(MODEL_PATH)\n",
    "with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(LABEL_ENCODER_PATH, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c6ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
