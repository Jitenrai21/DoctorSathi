{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "381fcb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 1: Import Libraries and Define Paths\n",
    "#\n",
    "# In this cell, we import the necessary libraries and define the file paths for the dataset, BioWordVec embeddings, and output files.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Paths to files (adjust as needed)\n",
    "DATA_PATH = \"DiseaseAndSymptoms.csv\"\n",
    "BIOWORDVEC_PATH = r\"C:\\Users\\ACER\\Downloads\\BioWordVec_PubMed_MIMICIII_d200.vec.bin\" # Path to BioWordVec embeddings in .bin format\n",
    "MODEL_PATH = \"biowordvec_diagnosis_model.keras\"\n",
    "TOKENIZER_PATH = \"symptom_tokenizer.pkl\"\n",
    "LABEL_ENCODER_PATH = \"label_encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e63739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Training dataset size: 3936, Test dataset size: 984\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 2: Load and Preprocess the Dataset\n",
    "#\n",
    "# This cell loads the dataset (`DiseaseAndSymptoms.csv`), preprocesses it, and splits it into train/test sets.\n",
    "\n",
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"\n",
    "    Load the dataset, preprocess symptoms, and split into train/test sets.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Peptic ulcer diseae\", \"Peptic ulcer disease\")\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Dimorphic hemmorhoids(piles)\", \"Dimorphic hemorrhoids (piles)\")\n",
    "    \n",
    "    # Replace underscores with spaces in column names and data\n",
    "    data.columns = [col.replace(\"_\", \" \") for col in data.columns]\n",
    "    data = data.apply(lambda x: x.str.replace(\"_\", \" \") if x.dtype == \"object\" else x)\n",
    "    \n",
    "    # Combine all symptoms into a single string per row\n",
    "    symptom_cols = [col for col in data.columns if \"Symptom\" in col]\n",
    "    data[\"Symptoms\"] = data[symptom_cols].apply(\n",
    "        lambda row: \" \".join(sorted(set([s.strip() for s in row if pd.notna(s)]))), axis=1\n",
    "    )\n",
    "    \n",
    "    # Drop rows with empty symptoms\n",
    "    data = data[data[\"Symptoms\"].str.strip() != \"\"]\n",
    "    \n",
    "    # Split into train and test sets (before augmentation)\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return train_data[[\"Disease\", \"Symptoms\"]], test_data[[\"Disease\", \"Symptoms\"]]\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "train_data, test_data = load_and_preprocess_data(DATA_PATH)\n",
    "print(f\"Training dataset size: {len(train_data)}, Test dataset size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c3d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training data...\n",
      "Original training dataset size: 3936, Augmented training dataset size: 4756\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 3: Augment the Training Dataset\n",
    "#\n",
    "# This cell augments the training dataset by creating synthetic symptom combinations for each disease.\n",
    "\n",
    "def augment_data(data, samples_per_disease=20):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating synthetic symptom combinations with co-occurrence patterns.\n",
    "    \"\"\"\n",
    "    augmented_data = data.copy()\n",
    "    diseases = data[\"Disease\"].unique()\n",
    "    \n",
    "    # For each disease, create synthetic samples\n",
    "    for disease in diseases:\n",
    "        disease_rows = data[data[\"Disease\"] == disease]\n",
    "        all_symptoms = set()\n",
    "        for symptoms in disease_rows[\"Symptoms\"]:\n",
    "            all_symptoms.update(symptoms.split())\n",
    "        all_symptoms = list(all_symptoms)\n",
    "        \n",
    "        # Generate synthetic samples\n",
    "        for _ in range(samples_per_disease):\n",
    "            # Randomly select 2 to min(len(all_symptoms), 5) symptoms\n",
    "            num_symptoms = np.random.randint(2, min(len(all_symptoms) + 1, 6))\n",
    "            selected_symptoms = np.random.choice(all_symptoms, size=num_symptoms, replace=False)\n",
    "            synthetic_symptoms = \" \".join(sorted(selected_symptoms))\n",
    "            augmented_data = pd.concat([augmented_data, pd.DataFrame({\n",
    "                \"Disease\": [disease],\n",
    "                \"Symptoms\": [synthetic_symptoms]\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "print(\"Augmenting training data...\")\n",
    "augmented_train_data = augment_data(train_data, samples_per_disease=20)\n",
    "print(f\"Original training dataset size: {len(train_data)}, Augmented training dataset size: {len(augmented_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c5616d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for training...\n",
      "Training samples: 3804, Validation samples: 952, Test samples: 984\n",
      "Vocabulary size: 209, Number of classes: 41\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 4: Prepare Data for Training\n",
    "#\n",
    "# This cell tokenizes the symptoms, encodes the diseases, and prepares the training, validation, and test sets.\n",
    "\n",
    "def prepare_data(train_data, test_data, max_len=20):\n",
    "    \"\"\"\n",
    "    Tokenize symptoms, encode diseases, and prepare training, validation, and test data.\n",
    "    \"\"\"\n",
    "    # Tokenize symptoms\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data[\"Symptoms\"])\n",
    "    \n",
    "    # Convert symptoms to sequences\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_data[\"Symptoms\"])\n",
    "    train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    test_sequences = tokenizer.texts_to_sequences(test_data[\"Symptoms\"])\n",
    "    test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    # Encode diseases\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels = label_encoder.fit_transform(train_data[\"Disease\"])\n",
    "    test_labels = label_encoder.transform(test_data[\"Disease\"])\n",
    "    \n",
    "    # Split augmented training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_sequences, train_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Compute class weights to handle class imbalance\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, test_sequences, test_labels, tokenizer, label_encoder, len(tokenizer.word_index) + 1, class_weight_dict\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "X_train, X_val, y_train, y_val, X_test, y_test, tokenizer, label_encoder, vocab_size, class_weight_dict = prepare_data(augmented_train_data, test_data)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}, Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "682df10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BioWordVec embeddings...\n",
      "Loading BioWordVec embeddings from .bin file...\n",
      "Loaded 16545452 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 5: Load BioWordVec Embeddings\n",
    "#\n",
    "# This cell loads the BioWordVec embeddings from the .bin file using gensim's KeyedVectors.\n",
    "\n",
    "def load_biowordvec_embeddings(biowordvec_path):\n",
    "    \"\"\"\n",
    "    Load BioWordVec embeddings from a .bin file using gensim.\n",
    "    \"\"\"\n",
    "    print(\"Loading BioWordVec embeddings from .bin file...\")\n",
    "    embeddings = KeyedVectors.load_word2vec_format(biowordvec_path, binary=True)\n",
    "    return embeddings\n",
    "\n",
    "print(\"Loading BioWordVec embeddings...\")\n",
    "embeddings_index = load_biowordvec_embeddings(BIOWORDVEC_PATH)\n",
    "print(f\"Loaded {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7afdae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding matrix...\n",
      "Embedding matrix shape: (209, 200)\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 6: Create Embedding Matrix\n",
    "#\n",
    "# This cell creates an embedding matrix using the BioWordVec embeddings for the tokenized vocabulary.\n",
    "\n",
    "def create_embedding_matrix(tokenizer, embeddings_index, vocab_size, embedding_dim=200):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the model using BioWordVec embeddings.\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            # Word not in BioWordVec vocabulary, leave as zero vector\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "print(\"Creating embedding matrix...\")\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, embeddings_index, vocab_size)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc68db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and training model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │        \u001b[38;5;34m41,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,800</span> (163.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m41,800\u001b[0m (163.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,800</span> (163.28 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m41,800\u001b[0m (163.28 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.0936 - loss: 3.4105 - val_accuracy: 0.7584 - val_loss: 1.6513\n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3310 - loss: 2.3976 - val_accuracy: 0.8414 - val_loss: 1.0451\n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4178 - loss: 2.0330 - val_accuracy: 0.8739 - val_loss: 0.7952\n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.4889 - loss: 1.7868 - val_accuracy: 0.8887 - val_loss: 0.6796\n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5296 - loss: 1.6077 - val_accuracy: 0.8992 - val_loss: 0.6008\n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5467 - loss: 1.5591 - val_accuracy: 0.8971 - val_loss: 0.5459\n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6002 - loss: 1.3401 - val_accuracy: 0.9034 - val_loss: 0.4943\n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.6014 - loss: 1.3209 - val_accuracy: 0.9055 - val_loss: 0.4566\n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6192 - loss: 1.2552 - val_accuracy: 0.9097 - val_loss: 0.4435\n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6254 - loss: 1.2184 - val_accuracy: 0.9139 - val_loss: 0.4087\n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6415 - loss: 1.1610 - val_accuracy: 0.9118 - val_loss: 0.4095\n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6396 - loss: 1.1558 - val_accuracy: 0.9160 - val_loss: 0.3759\n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6495 - loss: 1.1390 - val_accuracy: 0.9202 - val_loss: 0.3653\n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6669 - loss: 1.0743 - val_accuracy: 0.9212 - val_loss: 0.3530\n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6655 - loss: 1.0745 - val_accuracy: 0.9223 - val_loss: 0.3419\n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6709 - loss: 1.0225 - val_accuracy: 0.9244 - val_loss: 0.3422\n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6811 - loss: 1.0283 - val_accuracy: 0.9275 - val_loss: 0.3171\n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6959 - loss: 0.9716 - val_accuracy: 0.9296 - val_loss: 0.3185\n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6834 - loss: 1.0147 - val_accuracy: 0.9275 - val_loss: 0.3119\n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7039 - loss: 0.9140 - val_accuracy: 0.9307 - val_loss: 0.3061\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 7: Build and Train the Model\n",
    "#\n",
    "# This cell builds a simpler dense model with BioWordVec embeddings and trains it on the augmented dataset.\n",
    "\n",
    "def build_and_train_model(X_train, X_val, y_train, y_val, vocab_size, embedding_matrix, num_classes, class_weight_dict, max_len=20, embedding_dim=200):\n",
    "    \"\"\"\n",
    "    Build and train a simpler dense model with BioWordVec embeddings.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=True),  # Fine-tune embeddings\n",
    "        Flatten(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    # Add early stopping\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model with class weights\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Building and training model...\")\n",
    "model, history = build_and_train_model(\n",
    "    X_train, X_val, y_train, y_val, vocab_size, embedding_matrix, num_classes, class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e4d01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "Test Loss: 0.0118, Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 8: Evaluate on Test Set\n",
    "#\n",
    "# This cell evaluates the model on the separate test set.\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da4cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model, tokenizer, and label encoder...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 9: Save the Model, Tokenizer, and Label Encoder\n",
    "#\n",
    "# This cell saves the trained model, tokenizer, and label encoder for use in the inference script (`main.py`).\n",
    "\n",
    "print(\"Saving model, tokenizer, and label encoder...\")\n",
    "model.save(MODEL_PATH)\n",
    "with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(LABEL_ENCODER_PATH, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c6ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
