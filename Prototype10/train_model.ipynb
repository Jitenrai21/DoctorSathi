{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fcb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Paths to files (adjust as needed)\n",
    "DATA_PATH = \"DiseaseAndSymptoms.csv\"\n",
    "BIOWORDVEC_PATH = \"BioWordVec_PubMed_MIMICIII_d200.txt\"  # Path to BioWordVec embeddings\n",
    "MODEL_PATH = \"biowordvec_diagnosis_model.keras\"\n",
    "TOKENIZER_PATH = \"symptom_tokenizer.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 2: Load and Preprocess the Dataset\n",
    "#\n",
    "# This cell loads the dataset (`DiseaseAndSymptoms.csv`) and preprocesses it by combining symptoms into a single string per row and fixing disease name typos.\n",
    "\n",
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"\n",
    "    Load the dataset, preprocess symptoms, and encode diseases.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Peptic ulcer diseae\", \"Peptic ulcer disease\")\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Dimorphic hemmorhoids(piles)\", \"Dimorphic hemorrhoids (piles)\")\n",
    "    \n",
    "    # Replace underscores with spaces in column names and data\n",
    "    data.columns = [col.replace(\"_\", \" \") for col in data.columns]\n",
    "    data = data.apply(lambda x: x.str.replace(\"_\", \" \") if x.dtype == \"object\" else x)\n",
    "    \n",
    "    # Combine all symptoms into a single string per row\n",
    "    symptom_cols = [col for col in data.columns if \"Symptom\" in col]\n",
    "    data[\"Symptoms\"] = data[symptom_cols].apply(\n",
    "        lambda row: \" \".join(sorted(set([s.strip() for s in row if pd.notna(s)]))), axis=1\n",
    "    )\n",
    "    \n",
    "    # Drop rows with empty symptoms\n",
    "    data = data[data[\"Symptoms\"].str.strip() != \"\"]\n",
    "    \n",
    "    return data[[\"Disease\", \"Symptoms\"]]\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "data = load_and_preprocess_data(DATA_PATH)\n",
    "print(f\"Loaded dataset with {len(data)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 3: Augment the Dataset\n",
    "#\n",
    "# This cell augments the dataset by creating synthetic symptom combinations for each disease to increase data diversity.\n",
    "\n",
    "def augment_data(data):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating synthetic symptom combinations.\n",
    "    \"\"\"\n",
    "    augmented_data = data.copy()\n",
    "    diseases = data[\"Disease\"].unique()\n",
    "    \n",
    "    # For each disease, create synthetic samples by combining symptoms\n",
    "    for disease in diseases:\n",
    "        disease_rows = data[data[\"Disease\"] == disease]\n",
    "        all_symptoms = set()\n",
    "        for symptoms in disease_rows[\"Symptoms\"]:\n",
    "            all_symptoms.update(symptoms.split())\n",
    "        \n",
    "        # Generate synthetic samples (e.g., 5 new samples per disease)\n",
    "        for _ in range(5):\n",
    "            # Randomly select 2-5 symptoms\n",
    "            num_symptoms = np.random.randint(2, 6)\n",
    "            selected_symptoms = np.random.choice(list(all_symptoms), size=num_symptoms, replace=False)\n",
    "            synthetic_symptoms = \" \".join(sorted(selected_symptoms))\n",
    "            augmented_data = pd.concat([augmented_data, pd.DataFrame({\n",
    "                \"Disease\": [disease],\n",
    "                \"Symptoms\": [synthetic_symptoms]\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "print(\"Augmenting data...\")\n",
    "augmented_data = augment_data(data)\n",
    "print(f\"Original dataset size: {len(data)}, Augmented dataset size: {len(augmented_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5616d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 4: Prepare Data for Training\n",
    "#\n",
    "# This cell tokenizes the symptoms, encodes the diseases, and splits the data into training and validation sets.\n",
    "\n",
    "def prepare_data(data, max_len=20):\n",
    "    \"\"\"\n",
    "    Tokenize symptoms, encode diseases, and prepare training data.\n",
    "    \"\"\"\n",
    "    # Tokenize symptoms\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data[\"Symptoms\"])\n",
    "    \n",
    "    # Convert symptoms to sequences\n",
    "    symptom_sequences = tokenizer.texts_to_sequences(data[\"Symptoms\"])\n",
    "    symptom_sequences = pad_sequences(symptom_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    # Encode diseases\n",
    "    label_encoder = LabelEncoder()\n",
    "    disease_labels = label_encoder.fit_transform(data[\"Disease\"])\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        symptom_sequences, disease_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, tokenizer, label_encoder, len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "X_train, X_val, y_train, y_val, tokenizer, label_encoder, vocab_size = prepare_data(augmented_data)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}, Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682df10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 5: Load BioWordVec Embeddings\n",
    "#\n",
    "# This cell loads the BioWordVec embeddings from the .bin file using gensim's KeyedVectors.\n",
    "\n",
    "def load_biowordvec_embeddings(biowordvec_path):\n",
    "    \"\"\"\n",
    "    Load BioWordVec embeddings from a .bin file using gensim.\n",
    "    \"\"\"\n",
    "    print(\"Loading BioWordVec embeddings from .bin file...\")\n",
    "    embeddings = KeyedVectors.load_word2vec_format(biowordvec_path, binary=True)\n",
    "    return embeddings\n",
    "\n",
    "print(\"Loading BioWordVec embeddings...\")\n",
    "embeddings_index = load_biowordvec_embeddings(BIOWORDVEC_PATH)\n",
    "print(f\"Loaded {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7afdae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 6: Create Embedding Matrix\n",
    "#\n",
    "# This cell creates an embedding matrix using the BioWordVec embeddings for the tokenized vocabulary.\n",
    "\n",
    "def create_embedding_matrix(tokenizer, embeddings_index, vocab_size, embedding_dim=200):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix for the model using BioWordVec embeddings.\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            # Word not in BioWordVec vocabulary, leave as zero vector\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "print(\"Creating embedding matrix...\")\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, embeddings_index, vocab_size)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 7: Build and Train the Model\n",
    "#\n",
    "# This cell builds an LSTM model with BioWordVec embeddings and trains it on the augmented dataset.\n",
    "\n",
    "def build_and_train_model(X_train, X_val, y_train, y_val, vocab_size, embedding_matrix, num_classes, max_len=20, embedding_dim=200):\n",
    "    \"\"\"\n",
    "    Build and train an LSTM model with BioWordVec embeddings.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Building and training model...\")\n",
    "model, history = build_and_train_model(\n",
    "    X_train, X_val, y_train, y_val, vocab_size, embedding_matrix, num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 8: Save the Model and Tokenizer\n",
    "#\n",
    "# This cell saves the trained model and tokenizer for use in the inference script (`main.py`).\n",
    "\n",
    "print(\"Saving model and tokenizer...\")\n",
    "model.save(MODEL_PATH)\n",
    "with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
