{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4920\n",
      "Number of classes: 41\n",
      "Class distribution:\n",
      " Disease\n",
      "Fungal infection                           120\n",
      "Hepatitis C                                120\n",
      "Hepatitis E                                120\n",
      "Alcoholic hepatitis                        120\n",
      "Tuberculosis                               120\n",
      "Common Cold                                120\n",
      "Pneumonia                                  120\n",
      "Dimorphic hemorrhoids (piles)              120\n",
      "Heart attack                               120\n",
      "Varicose veins                             120\n",
      "Hypothyroidism                             120\n",
      "Hyperthyroidism                            120\n",
      "Hypoglycemia                               120\n",
      "Osteoarthristis                            120\n",
      "Arthritis                                  120\n",
      "(vertigo) Paroymsal  Positional Vertigo    120\n",
      "Acne                                       120\n",
      "Urinary tract infection                    120\n",
      "Psoriasis                                  120\n",
      "Hepatitis D                                120\n",
      "Hepatitis B                                120\n",
      "Allergy                                    120\n",
      "hepatitis A                                120\n",
      "GERD                                       120\n",
      "Chronic cholestasis                        120\n",
      "Drug Reaction                              120\n",
      "Peptic ulcer disease                       120\n",
      "AIDS                                       120\n",
      "Diabetes                                   120\n",
      "Gastroenteritis                            120\n",
      "Bronchial Asthma                           120\n",
      "Hypertension                               120\n",
      "Migraine                                   120\n",
      "Cervical spondylosis                       120\n",
      "Paralysis (brain hemorrhage)               120\n",
      "Jaundice                                   120\n",
      "Malaria                                    120\n",
      "Chicken pox                                120\n",
      "Dengue                                     120\n",
      "Typhoid                                    120\n",
      "Impetigo                                   120\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "input_file = \"DiseaseAndSymptoms.csv\"\n",
    "data = pd.read_csv(input_file)\n",
    "data.columns = [col.replace(\"_\", \" \") for col in data.columns]\n",
    "data = data.apply(lambda x: x.str.replace(\"_\", \" \") if x.dtype == \"object\" else x)\n",
    "data[\"Disease\"] = data[\"Disease\"].replace(\"Peptic ulcer diseae\", \"Peptic ulcer disease\")\n",
    "data[\"Disease\"] = data[\"Disease\"].replace(\"Dimorphic hemmorhoids(piles)\", \"Dimorphic hemorrhoids (piles)\")\n",
    "\n",
    "# Clean and combine symptoms\n",
    "symptom_cols = [col for col in data.columns if \"Symptom\" in col]\n",
    "data[\"Symptoms\"] = data[symptom_cols].apply(\n",
    "    lambda row: \" \".join(sorted(set([s.strip() for s in row if pd.notna(s)]))), axis=1\n",
    ")\n",
    "\n",
    "# Verify data\n",
    "print(f\"Number of rows: {len(data)}\")\n",
    "disease_list = sorted(data[\"Disease\"].unique())\n",
    "num_classes = len(disease_list)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(\"Class distribution:\\n\", data[\"Disease\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3936, Validation samples: 984\n"
     ]
    }
   ],
   "source": [
    "# Split data (stratified)\n",
    "X_train, X_val, y_train_labels, y_val_labels = train_test_split(\n",
    "    data[\"Symptoms\"], data[\"Disease\"], test_size=0.2, random_state=42, stratify=data[\"Disease\"]\n",
    ")\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (3936, 41), Validation labels shape: (984, 41)\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to categorical\n",
    "y_train = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_train_labels], num_classes=num_classes)\n",
    "y_val = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_val_labels], num_classes=num_classes)\n",
    "print(f\"Training labels shape: {y_train.shape}, Validation labels shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented training samples: 256584, Validation samples: 64146\n",
      "Max sequence length: 30\n",
      "Training labels shape: (256584, 41), Validation labels shape: (64146, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\gitClones\\DoctorSathi\\env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,289</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m200\u001b[0m)        │        \u001b[38;5;34m41,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m51,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m)             │         \u001b[38;5;34m5,289\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,441</span> (513.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m131,441\u001b[0m (513.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,441</span> (513.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m131,441\u001b[0m (513.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load BioWordVec embeddings\n",
    "word2vec_path = r\"C:\\Users\\ACER\\Downloads\\bio_embedding_extrinsic.bin\"\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"DiseaseAndSymptoms.csv\")\n",
    "data.columns = [col.replace(\"_\", \" \") for col in data.columns]\n",
    "data = data.apply(lambda x: x.str.replace(\"_\", \" \") if x.dtype == \"object\" else x)\n",
    "symptom_cols = [col for col in data.columns if \"Symptom\" in col]\n",
    "data[\"Symptoms\"] = data[symptom_cols].apply(\n",
    "    lambda row: \" \".join(sorted(set([s.strip() for s in row if pd.notna(s)]))), axis=1\n",
    ")\n",
    "\n",
    "# Augment data with partial symptom combinations\n",
    "import random\n",
    "\n",
    "def generate_partial_symptoms(symptom_text, min_symptoms=2):\n",
    "    symptoms = symptom_text.split()\n",
    "    partial_combinations = []\n",
    "    for n in range(min_symptoms, len(symptoms) + 1):\n",
    "        for i in range(5):\n",
    "            partial = \" \".join(sorted(random.sample(symptoms, n)))\n",
    "            partial_combinations.append(partial)\n",
    "    return partial_combinations\n",
    "\n",
    "augmented_data = []\n",
    "for idx, row in data.iterrows():\n",
    "    symptom_text = row[\"Symptoms\"]\n",
    "    disease = row[\"Disease\"]\n",
    "    augmented_data.append((symptom_text, disease))\n",
    "    partial_symptoms = generate_partial_symptoms(symptom_text)\n",
    "    for partial in partial_symptoms:\n",
    "        augmented_data.append((partial, disease))\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=[\"Symptoms\", \"Disease\"])\n",
    "\n",
    "# Split augmented data\n",
    "X_train, X_val, y_train_labels, y_val_labels = train_test_split(\n",
    "    augmented_df[\"Symptoms\"], augmented_df[\"Disease\"], test_size=0.2, random_state=42, stratify=augmented_df[\"Disease\"]\n",
    ")\n",
    "print(f\"Augmented training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Tokenize symptoms\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(augmented_df[\"Symptoms\"])\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(seq) for seq in X_train_seq)\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\")\n",
    "X_val_seq = pad_sequences(X_val_seq, maxlen=max_len, padding=\"post\")\n",
    "print(f\"Max sequence length: {max_len}\")\n",
    "\n",
    "# Convert labels to categorical\n",
    "disease_list = sorted(data[\"Disease\"].unique())\n",
    "num_classes = len(disease_list)\n",
    "y_train = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_train_labels], num_classes=num_classes)\n",
    "y_val = tf.keras.utils.to_categorical([disease_list.index(d) for d in y_val_labels], num_classes=num_classes)\n",
    "print(f\"Training labels shape: {y_train.shape}, Validation labels shape: {y_val.shape}\")\n",
    "\n",
    "# Create embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = word_vectors.vector_size  # 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_vectors:\n",
    "        embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "# Build model with fine-tuned embeddings\n",
    "inputs = Input(shape=(max_len,))\n",
    "x = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True)(inputs)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m8004/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8335 - loss: 0.6020\n",
      "Epoch 1: val_accuracy improved from -inf to 0.95294, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.8337 - loss: 0.6013 - val_accuracy: 0.9529 - val_loss: 0.1281\n",
      "Epoch 2/20\n",
      "\u001b[1m8010/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9497 - loss: 0.1474\n",
      "Epoch 2: val_accuracy improved from 0.95294 to 0.95892, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 3ms/step - accuracy: 0.9497 - loss: 0.1474 - val_accuracy: 0.9589 - val_loss: 0.1107\n",
      "Epoch 3/20\n",
      "\u001b[1m8002/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9530 - loss: 0.1343\n",
      "Epoch 3: val_accuracy improved from 0.95892 to 0.95992, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.9530 - loss: 0.1343 - val_accuracy: 0.9599 - val_loss: 0.1053\n",
      "Epoch 4/20\n",
      "\u001b[1m8017/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1265\n",
      "Epoch 4: val_accuracy improved from 0.95992 to 0.96062, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.9553 - loss: 0.1265 - val_accuracy: 0.9606 - val_loss: 0.1080\n",
      "Epoch 5/20\n",
      "\u001b[1m8005/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9560 - loss: 0.1234\n",
      "Epoch 5: val_accuracy improved from 0.96062 to 0.96176, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.9560 - loss: 0.1234 - val_accuracy: 0.9618 - val_loss: 0.1031\n",
      "Epoch 6/20\n",
      "\u001b[1m8003/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1204\n",
      "Epoch 6: val_accuracy improved from 0.96176 to 0.96282, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1204 - val_accuracy: 0.9628 - val_loss: 0.0970\n",
      "Epoch 7/20\n",
      "\u001b[1m8007/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1184\n",
      "Epoch 7: val_accuracy did not improve from 0.96282\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1184 - val_accuracy: 0.9614 - val_loss: 0.1064\n",
      "Epoch 8/20\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1178\n",
      "Epoch 8: val_accuracy did not improve from 0.96282\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1178 - val_accuracy: 0.9625 - val_loss: 0.0968\n",
      "Epoch 9/20\n",
      "\u001b[1m8007/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9579 - loss: 0.1159\n",
      "Epoch 9: val_accuracy improved from 0.96282 to 0.96344, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3ms/step - accuracy: 0.9579 - loss: 0.1159 - val_accuracy: 0.9634 - val_loss: 0.0959\n",
      "Epoch 10/20\n",
      "\u001b[1m8013/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9584 - loss: 0.1151\n",
      "Epoch 10: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9584 - loss: 0.1151 - val_accuracy: 0.9623 - val_loss: 0.0973\n",
      "Epoch 11/20\n",
      "\u001b[1m8007/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9585 - loss: 0.1137\n",
      "Epoch 11: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.9585 - loss: 0.1137 - val_accuracy: 0.9624 - val_loss: 0.0968\n",
      "Epoch 12/20\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1133\n",
      "Epoch 12: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1133 - val_accuracy: 0.9613 - val_loss: 0.0992\n",
      "Epoch 13/20\n",
      "\u001b[1m8007/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1148\n",
      "Epoch 13: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1148 - val_accuracy: 0.9632 - val_loss: 0.0936\n",
      "Epoch 14/20\n",
      "\u001b[1m8013/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1123\n",
      "Epoch 14: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1123 - val_accuracy: 0.9624 - val_loss: 0.1030\n",
      "Epoch 15/20\n",
      "\u001b[1m8012/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9583 - loss: 0.1137\n",
      "Epoch 15: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9583 - loss: 0.1137 - val_accuracy: 0.9607 - val_loss: 0.1003\n",
      "Epoch 16/20\n",
      "\u001b[1m8009/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1145\n",
      "Epoch 16: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1145 - val_accuracy: 0.9628 - val_loss: 0.0959\n",
      "Epoch 17/20\n",
      "\u001b[1m8007/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1147\n",
      "Epoch 17: val_accuracy did not improve from 0.96344\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1147 - val_accuracy: 0.9630 - val_loss: 0.0943\n",
      "Epoch 18/20\n",
      "\u001b[1m8010/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1133\n",
      "Epoch 18: val_accuracy improved from 0.96344 to 0.96352, saving model to biowordvec_diagnosis_model.keras\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1133 - val_accuracy: 0.9635 - val_loss: 0.0930\n",
      "Epoch 19/20\n",
      "\u001b[1m8006/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9575 - loss: 0.1150\n",
      "Epoch 19: val_accuracy did not improve from 0.96352\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1150 - val_accuracy: 0.9599 - val_loss: 0.1017\n",
      "Epoch 20/20\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9584 - loss: 0.1132\n",
      "Epoch 20: val_accuracy did not improve from 0.96352\n",
      "\u001b[1m8019/8019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 3ms/step - accuracy: 0.9584 - loss: 0.1132 - val_accuracy: 0.9634 - val_loss: 0.0974\n",
      "Restoring model weights from the end of the best epoch: 18.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\"biowordvec_diagnosis_model.keras\", monitor=\"val_accuracy\", save_best_only=True, mode=\"max\", verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_val_seq, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer to symptom_tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the tokenizer for inference\n",
    "with open(\"symptom_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Saved tokenizer to symptom_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
