{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 1: Import Libraries and Define Paths\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Paths to files (adjust as needed)\n",
    "DATA_PATH = \"DiseaseAndSymptoms.csv\"\n",
    "MODEL_PATH = \"diagnosis_model.keras\"\n",
    "TOKENIZER_PATH = \"symptom_tokenizer.pkl\"\n",
    "LABEL_ENCODER_PATH = \"label_encoder.pkl\"\n",
    "\n",
    "print(\"Libraries imported and paths defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 2: Load and Preprocess the Dataset\n",
    "\n",
    "def load_and_preprocess_data(data_path):\n",
    "    \"\"\"\n",
    "    Load the dataset, preprocess symptoms, and split into train/val/test sets.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_path)\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Peptic ulcer diseae\", \"Peptic ulcer disease\")\n",
    "    data[\"Disease\"] = data[\"Disease\"].replace(\"Dimorphic hemmorhoids(piles)\", \"Dimorphic hemorrhoids (piles)\")\n",
    "    \n",
    "    data.columns = [col.replace(\"_\", \" \") for col in data.columns]\n",
    "    data = data.apply(lambda x: x.str.replace(\"_\", \" \") if x.dtype == \"object\" else x)\n",
    "    \n",
    "    symptom_cols = [col for col in data.columns if \"Symptom\" in col]\n",
    "    data[\"Symptoms\"] = data[symptom_cols].apply(\n",
    "        lambda row: \" \".join(sorted(set([s.strip() for s in row if pd.notna(s)]))), axis=1\n",
    "    )\n",
    "    \n",
    "    data = data[data[\"Symptoms\"].str.strip() != \"\"]\n",
    "    \n",
    "    # Deduplicate the dataset\n",
    "    data = data.drop_duplicates(subset=[\"Symptoms\", \"Disease\"])\n",
    "    print(f\"Dataset size after deduplication: {len(data)}\")\n",
    "    \n",
    "    # Split into train+val and test sets (stratified)\n",
    "    train_val_data, test_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data[\"Disease\"])\n",
    "    \n",
    "    # Split train+val into train and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=0.2, random_state=42, stratify=train_val_data[\"Disease\"])\n",
    "    \n",
    "    # Check for overlap\n",
    "    train_symptoms = set(train_data[\"Symptoms\"])\n",
    "    test_symptoms = set(test_data[\"Symptoms\"])\n",
    "    overlap = train_symptoms.intersection(test_symptoms)\n",
    "    print(f\"Number of overlapping symptom strings between train and test: {len(overlap)}\")\n",
    "    if overlap:\n",
    "        print(\"Sample overlapping symptoms:\", list(overlap)[:5])\n",
    "    \n",
    "    # Check class distribution\n",
    "    print(\"Test set class distribution:\\n\", test_data[\"Disease\"].value_counts())\n",
    "    \n",
    "    return train_data[[\"Disease\", \"Symptoms\"]], val_data[[\"Disease\", \"Symptoms\"]], test_data[[\"Disease\", \"Symptoms\"]]\n",
    "\n",
    "print(\"Loading and preprocessing data...\")\n",
    "train_data, val_data, test_data = load_and_preprocess_data(DATA_PATH)\n",
    "print(f\"Training dataset size: {len(train_data)}, Validation dataset size: {len(val_data)}, Test dataset size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6613f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 3: Augment the Training Dataset\n",
    "\n",
    "def augment_data(data, samples_per_disease=20):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating synthetic symptom combinations with co-occurrence patterns.\n",
    "    \"\"\"\n",
    "    augmented_data = data.copy()\n",
    "    diseases = data[\"Disease\"].unique()\n",
    "    \n",
    "    for disease in diseases:\n",
    "        disease_rows = data[data[\"Disease\"] == disease]\n",
    "        all_symptoms = set()\n",
    "        for symptoms in disease_rows[\"Symptoms\"]:\n",
    "            all_symptoms.update(symptoms.split())\n",
    "        all_symptoms = list(all_symptoms)\n",
    "        \n",
    "        # Calculate symptom co-occurrence\n",
    "        symptom_counts = {}\n",
    "        for symptoms in disease_rows[\"Symptoms\"]:\n",
    "            symptom_list = symptoms.split()\n",
    "            for i, s1 in enumerate(symptom_list):\n",
    "                if s1 not in symptom_counts:\n",
    "                    symptom_counts[s1] = {}\n",
    "                for s2 in symptom_list[i+1:]:\n",
    "                    if s2 not in symptom_counts[s1]:\n",
    "                        symptom_counts[s1][s2] = 0\n",
    "                    symptom_counts[s1][s2] += 1\n",
    "        \n",
    "        # Normalize to get probabilities\n",
    "        symptom_probs = {}\n",
    "        for s1 in symptom_counts:\n",
    "            total = sum(symptom_counts[s1].values())\n",
    "            if total > 0:\n",
    "                symptom_probs[s1] = {s2: count/total for s2, count in symptom_counts[s1].items()}\n",
    "        \n",
    "        # Generate synthetic samples\n",
    "        for _ in range(samples_per_disease):\n",
    "            num_symptoms = np.random.randint(2, min(len(all_symptoms) + 1, 6))\n",
    "            selected_symptoms = []\n",
    "            current_symptom = np.random.choice(all_symptoms)\n",
    "            selected_symptoms.append(current_symptom)\n",
    "            \n",
    "            for _ in range(num_symptoms - 1):\n",
    "                if current_symptom in symptom_probs and symptom_probs[current_symptom]:\n",
    "                    next_symptom_probs = symptom_probs[current_symptom]\n",
    "                    next_symptom = np.random.choice(\n",
    "                        list(next_symptom_probs.keys()),\n",
    "                        p=list(next_symptom_probs.values())\n",
    "                    )\n",
    "                    selected_symptoms.append(next_symptom)\n",
    "                    current_symptom = next_symptom\n",
    "                else:\n",
    "                    remaining_symptoms = [s for s in all_symptoms if s not in selected_symptoms]\n",
    "                    if remaining_symptoms:\n",
    "                        next_symptom = np.random.choice(remaining_symptoms)\n",
    "                        selected_symptoms.append(next_symptom)\n",
    "                        current_symptom = next_symptom\n",
    "            \n",
    "            synthetic_symptoms = \" \".join(sorted(set(selected_symptoms)))\n",
    "            augmented_data = pd.concat([augmented_data, pd.DataFrame({\n",
    "                \"Disease\": [disease],\n",
    "                \"Symptoms\": [synthetic_symptoms]\n",
    "            })], ignore_index=True)\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "print(\"Augmenting training data...\")\n",
    "augmented_train_data = augment_data(train_data, samples_per_disease=20)\n",
    "print(f\"Original training dataset size: {len(train_data)}, Augmented training dataset size: {len(augmented_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 4: Prepare Data for Training\n",
    "\n",
    "def prepare_data(train_data, val_data, test_data, max_len=20):\n",
    "    \"\"\"\n",
    "    Tokenize symptoms, encode diseases, and prepare training, validation, and test data.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data[\"Symptoms\"])\n",
    "    \n",
    "    train_sequences = tokenizer.texts_to_sequences(train_data[\"Symptoms\"])\n",
    "    train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    val_sequences = tokenizer.texts_to_sequences(val_data[\"Symptoms\"])\n",
    "    val_sequences = pad_sequences(val_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    test_sequences = tokenizer.texts_to_sequences(test_data[\"Symptoms\"])\n",
    "    test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    train_labels = label_encoder.fit_transform(train_data[\"Disease\"])\n",
    "    val_labels = label_encoder.transform(val_data[\"Disease\"])\n",
    "    test_labels = label_encoder.transform(test_data[\"Disease\"])\n",
    "    \n",
    "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    class_weight_dict = {k: np.sqrt(v) for k, v in class_weight_dict.items()}  # Soften weights\n",
    "    \n",
    "    return train_sequences, val_sequences, test_sequences, train_labels, val_labels, test_labels, tokenizer, label_encoder, len(tokenizer.word_index) + 1, class_weight_dict\n",
    "\n",
    "print(\"Preparing data for training...\")\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, tokenizer, label_encoder, vocab_size, class_weight_dict = prepare_data(augmented_train_data, val_data, test_data)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}, Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 5: Build and Train the Model\n",
    "\n",
    "def build_and_train_model(X_train, X_val, y_train, y_val, vocab_size, num_classes, class_weight_dict, max_len=20, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    Build and train a simple dense model with a trainable embedding layer.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_len, trainable=True),\n",
    "        Flatten(),\n",
    "        Dense(64, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Building and training model...\")\n",
    "model, history = build_and_train_model(\n",
    "    X_train, X_val, y_train, y_val, vocab_size, num_classes, class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739af2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 6: Evaluate on Test Set\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c548501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cell 7: Save the Model, Tokenizer, and Label Encoder\n",
    "\n",
    "print(\"Saving model, tokenizer, and label encoder...\")\n",
    "model.save(MODEL_PATH)\n",
    "with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "with open(LABEL_ENCODER_PATH, \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
